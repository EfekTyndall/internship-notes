The innovative "Mirai" robot control technology from Micropsi Industries was integrated as part of the project. This technology enables the automation of complex motion sequences in dynamic environments through an intuitive training process in which the robot learns directly through manual guidance.

## MIRAI Technology Overview

The MIRAI robot control system from Micropsi Industries enables the automation of complex motion sequences in dynamic process environments through hand-guided training of the robot. MIRAI is a controller for robot control based on machine learning that generates robot movements via sensory input (camera image, force/torque readings, other sensors) and thus solves complex tasks in real time.

Most automation solutions commonly used today are either programmed in a scripting language or "taught" with the help of a (robot) teach pendant and its user interface. In addition to these approaches, MIRAI also makes it possible to train a robot to solve complex tasks that require hand-eye coordination. To do this, MIRAI imitates the user's movements, which the user teaches the system by guiding the robot arm. The most important advantages of MIRAI compared to "classic" automation approaches are its ease of use: complex motion sequences can be realized and automated without extensive prior knowledge of automation technology, machine vision or programming. In addition, thanks to the underlying machine learning, MIRAI is able to deal with different sequence variants and dynamics within the task and its environment. Thanks to these properties, the MIRAI controller can handle a range of new automation tasks and problems that were previously impossible or could only be implemented with complicated and expensive solutions.

To "train" the robot, MIRAI must first be shown the movements. To do this, the robot is manually guided from the target position to the planned starting position(s). If possible, the entire area between the starting position(s) and the target position should be 'scanned' with the movement. We call the demonstrations of these "scanning movements" episodes. For more complex tasks, a sequence of movements is repeatedly demonstrated and recorded, whereby the robot is guided from different starting positions to the target position. The respective episodes are recorded and managed using our Android-based MIRAI training app. They are then converted by our cloud-based training server into a robot movement program based on machine vision. The result is a "MIRAI Skill". The trained skills ultimately enable the MIRAI controller to imitate the movements and sequences that were previously demonstrated, while recognizing the dynamics and variances that were the focus of the training in real time. the focus of the training in real time (in a closed control loop).

## The AI in MIRAI

A subset of artificial intelligence, machine learning refers to a field of study within applied mathematics that, using statistical inference, supplies computers with the ability to learn without being explicitly programmed. There are assorted kinds of machine learning. At Micropsi Industries, we do imitation learning. This is a variant of supervised learning, the goal of which is to get a computer to generate a correct answer to a problem by showing it examples of correct answers to similar problems. You would apply a supervised learning computer-algorithm if you wanted to, say, predict annual income based on the number of years of higher education someone has. The algorithm would do this by analyzing a raft of training data on education and income, discerning a pattern in that data, and then, from this pattern, predicting a person’s annual income when later fed only the education data. In imitation learning, the aim is to predict a correct answer in the form of behavior. The machine learns to make the most appropriate decision based on training data collected from human demonstrations.

With the MIRAI system, a robot’s behavior is acquired from what’s observed by the camera during human demonstrations. A robot behavior here could be a robot plugging a cable into a server rack hole or sniffing the soldering joint on the back of a refrigerator for leakage. During demonstrations, you take the MIRAI-equipped robot by its wrist and show it the action you want it to perform and the variance it may come across. 

As you do, the camera records the scene. The camera-recorded images are converted into data and transported, via the MIRAI controller, to the Micropsi Industries’ secure computing cloud. There, we run the data through a learning algorithm to prepare, or train, a policy, a mathematical model that guides the robot when it performs the action we want it to execute. At Micropsi Industries, we refer to this trained mathematical model as a “skill.” It instructs the robot how to behave in real time when it encounters variance while doing your desired action, in a completely self-contained manner without any more need for a network connection. Skills are like laws that tell the robot how to behave in any situation it encounters. That law is generated and fine-tuned based on the movements you show the system during training. Typically, one does a few training sessions to make sure a skill is robust, meaning it can handle any situation that comes up.

## Use Case

The Mirai robot control system is used for the precise task of gripping, picking up, and placing truck switches during the assembly process, ensuring they are correctly positioned within the bracket.

## Main Advantages Using MIRAI Technology Solution

Automate what was previously impossible to automate or was not economical to automate with the AI control MIRAI generates your robots movements directly and in real time using AI;

- Changes in the position, shape or color of a component.
- Changing lighting conditions.
- Dynamic environmental conditions.
- System flexibility retrained for new task any time.
- Multifunctional tool for demanding tasks.

## Validation

**Pro's:**
- Flexibility to combine objects/ materials in the same process.
- 3 axis motion (translation and rotation) option is helpful to record episodes.
- Easy to integrate with FANUC teach pendant program

**Con's:**
- Lack of accuracy (needs plenty of trainings to have a robust skill)
- Takes a long time to train and setup the system (ca. 4-5 hours)
- Vision scope limitations based on the camera (Camera Area)
- For FANUC integration dead man switch need to be pressed all the time during training process (T1 Mode).
- Force sensor is needed

## Components

- Fanuc CRX-10iA/L collaborative robot
- MIRAI robot controller
	- recording and storing training episodes the user performs 
	- receiving and storing trained skills from the Micropsi Industries training cloud; and 
	- generating and driving robot movements by executing trained skills in real time, based on vision and other sensory input.
- Ximea xiQ USB3 camera with Fujinon lens and ring light
- F/T Sensor
- Gripper

## Mirai Setup

[[Insert setup scheme here]]

For all of the various components to communicate to one another, an internal network need to be set up. The schematic shows the various components that a MIRAI based solution includes and how to configure the internal network

The MIRAI solution comprises of the following elements: 
- MIRAI robot controller creates sensor-based, real-time robot movements based on trained ‘skills’. 
- ‘MIRAI Training App’ is the primary user interface for the MIRAI controller to record training episodes and manage MIRAI skills. It is a mobile app for Android based tablet devices. 
- The Micropsi Industries cloud training service calculates MIRAI skills based on the recorded and uploaded training episodes. 
- The MIRAI software package enables the user to access the MIRAI skills and integrate these in FANUC Teach Pendant robot program flows

## Camera, F/T Sensor and Gripper

The Camera, force-torque sensor and gripper are all mounted on the tool flange of the robot.

[[Insert tool image with coordinate system here]]

## My Task:

1. Learn about Mirai Technology though given documentation
2. Learn how to operate FANUC CRX-10iA/L Robot installed with Mirai Technology:
	- Learn how to train a positioning skill for the robot to reach a target objects (switches)
	- Perform test in order to find the most effective way to create a robust skill for the required task
	- Create a demo for the assembly process, which covers gripping, picking and placing process of the switches into the bracket.
	- Create a documentation:
		- How to troubleshoot possible encountered problems

## Step by step on creating a Demo
1. Train picking skill as a positioning skill on MIRAI Micropsi App
2. Train placing skill as a positioning skill on MIRAI Mircropsi App
3. Create a program on the Fanuc CRX-10iA/L Robot's Teach Pendant and integrate the taught skills in the program (Each skill called as a Karel Program)

## Creating Position Skill

- Jog the robot to the chosen start position using robot's teach pendant.
- Disable the Tablet TP by tapping on “TP enable” button in the upper right corner.
- In the training App tap on the menu button in the upper left corner and select ‘Training’.
- Tap on "Add new skill" to open the "Skill Selection" form and fill out all of the necessary information, like "Skill Name", "Type of Skill" in this case "Positioning Skill" and "Number of Cameras".
- Tap "Next" to got to the second step "Skill Configuration"
- Choose the Robot, Force/Torque Sensor and Camera which are used for the "Skill"
- Specify how to camera is mounted. In this case is "Wrist Mounted"
- Under the "Axis Configuration" select on which translational and rotational axes should the robot move. The robot can be trained to move on all the three translational axes i.e., movement along the x, y or z-axis and rotate on all the three rotational axes i.e., around x, y or z-axis. De-selecting one or more axes will prevent the robot to move along or rotate around that axis.
- For the picking process the robot should move along x, y, z-axis and rotate around z-axis. Other unnecessary axes should be de-selected.
- When rotation are enabled, is it required to specify the respective tool in use. From the Tool dropdown, select a previous calibrated tool.
- Certain tasks require to displace the center of rotations to the TCP or some other reference point. This could be because the tool (see image below) is offset on all 3 axes or attached at a different angle, meaning that it is not parallel to the z-axis of tool flange. This step is unnecessary for the tool, because it is mounted parallel to the z-axis of the tool flange.
- Press ‘Next’ to finalize these settings and move to the third step of the skill creation process.
- The third step is to configure the camera settings. When adjusting the gain and exposure value in this screen, make sure to also adjust the focus and aperture physically on the camera itself. Aperture ranges from 1.4 to 16; a value of 8 is recommended. Adjust the focus of the lens so that in the camera feed the tip of the end effector is in focus.
- Once the settings on the camera lens are done, adjust the camera exposure time and gain in the training app to minimize over/under exposure. Guide the robot through the trajectories that the robot will likely take to check and adjust the camera settings to values that offer a good lighting range and sufficient contrast. Tap ‘Next’.
- In this section, you can set a safe and meaningful reference position for the tool center point by guiding the robot to the desired position and tapping ‘Save Position’. This is the position the robot can be moved back to when starting to record new episodes or test skills. Then tap on ‘Save Reference Position’.
- Skill configuration is now complete. Proceed to the next steps by tapping ‘Next’ to start recording your training episodes

